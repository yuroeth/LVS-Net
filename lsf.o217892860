Sender: LSF System <lsfadmin@eu-lo-s4-030>
Subject: Job 217892860: <python train.py --dataset cambridge_loc --scene KingsCollege --landmark line --experiment line0509/> in cluster <euler> Exited

Job <python train.py --dataset cambridge_loc --scene KingsCollege --landmark line --experiment line0509/> was submitted from host <eu-login-15> by user <yurohu> in cluster <euler> at Mon May  9 12:44:51 2022
Job was executed on host(s) <16*eu-lo-s4-030>, in queue <gpu.24h>, as user <yurohu> in cluster <euler> at Mon May  9 12:45:25 2022
</cluster/home/yurohu> was used as the home directory.
</cluster/home/yurohu/LVSNet/LVS-Net> was used as the working directory.
Started at Mon May  9 12:45:25 2022
Terminated at Mon May  9 12:46:26 2022
Results reported at Mon May  9 12:46:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python train.py --dataset cambridge_loc --scene KingsCollege --landmark line --experiment line0509/
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   22.85 sec.
    Max Memory :                                 3716 MB
    Average Memory :                             364.00 MB
    Total Requested Memory :                     16384.00 MB
    Delta Memory :                               12668.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                5
    Run time :                                   63 sec.
    Turnaround time :                            95 sec.

The output (if any) follows:

  0%|          | 0/610 [00:00<?, ?it/s]Train loss: 16.594356537|16.594356537:   0%|          | 0/610 [00:04<?, ?it/s]Train loss: 16.594356537|16.594356537:   0%|          | 1/610 [00:04<45:46,  4.51s/it]Train loss: 16.594356537|16.594356537:   0%|          | 1/610 [00:05<58:56,  5.81s/it]
Traceback (most recent call last):
  File "train.py", line 453, in <module>
    main()
  File "train.py", line 445, in main
    trainer.training(epoch)
  File "train.py", line 186, in training
    loss.backward()
  File "/cluster/home/yurohu/anaconda3/envs/vsnet/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/home/yurohu/anaconda3/envs/vsnet/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 254.00 MiB (GPU 0; 7.93 GiB total capacity; 7.32 GiB already allocated; 64.50 MiB free; 7.39 GiB reserved in total by PyTorch)
Params:
----------------------------------------------------
	message_prefix                :loc
	message                       :
	seed                          :1
	backbone                      :resnet
	out_stride                    :16
	sync_bn                       :False
	val_label_filter_threshsold   :20
	seg_decoder                   :v1
	seg_loss_type                 :embedding_v3
	seg_loss_margin               :0.5
	seg_k                         :2
	visualize_segmentation        :False
	vertex_decoder                :v2
	vertex_loss_type              :l1_loss
	vertex_loss_root              :1
	vertex_channel                :2
	vertex_loss_ratio             :3.0
	seg_loss_ratio                :1.0
	visualize_voting              :False
	visualize_landmark            :False
	train                         :True
	start_epoch                   :0
	eval_interval                 :5
	eval_epoch_begin              :80
	no_val                        :False
	use_pnp                       :False
	save_model                    :True
	val_batch_size                :1
	test_batch_size               :1
	shuffle                       :True
	num_workers                   :4
	base_dir                      :/cluster/project/infk/courses/252-0579-00L/group05/datasets/cambridge_line
	data_dir                      :
	color_map_filename            :id2colors.json
	optimizer                     :Adam
	weight_decay                  :0.0005
	momentum                      :0.9
	nesterov                      :False
	lr_scheduler                  :poly
	lr_step                       :20
	devices                       :0
	use_own_nn                    :True
	validation_debug              :False
	critical_params               :['dataset', 'scene', 'train_batch_size', 'epochs', 'lr', 'use_aug', 'seg_channel']
	resume                        :None
	resume_checkpoint             :
	checkname                     :landmarknet-resnet
	export_dir                    :logs
	log_tb_dir                    :logs
	experiment                    :line0509/
	checkpoint_dir                :ckpts
	best_model_name               :best_model.pth.tar
	write_json                    :True
	log_file                      :log.txt
	save_dir                      :/cluster/project/infk/courses/252-0579-00L/group05/models
	coding_book_filename          :channel(64)_isomap(k24)_cambridge_ShopFacade.json
	landmark                      :line
----------------------------------------------------
Critical Params:
	seg_channel                   :64
	epochs                        :100
	train_batch_size              :2
	use_aug                       :True
	dataset                       :cambridge
	scene                         :KingsCollege
	lr                            :0.0001
----------------------------------------------------
write opt to: /cluster/project/infk/courses/252-0579-00L/group05/models/cambridge_logs/line0509/dataset[cambridge]scene[KingsCollege]train_batch_size[2]epochs[100]lr[0.0001]use_aug[True]seg_channel[64]/config.json
logging to /cluster/project/infk/courses/252-0579-00L/group05/models/cambridge_logs/line0509/dataset[cambridge]scene[KingsCollege]train_batch_size[2]epochs[100]lr[0.0001]use_aug[True]seg_channel[64]/log.txt
Number of images in train: 1220
Number of images in test: 343
coding book size = torch.Size([518, 64])
Using poly LR Scheduler!
Starting Epoch: 0
Total Epoches: 100
=================================
training
=================================

=>Epoches 0, learning rate = 0.000100000,                 previous best = 0.000000000
